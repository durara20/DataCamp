{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dJ_dw = [[0.99845601 2.39507239]]\n",
      "dJ_db = 0.001455578136784208\n",
      "cost = 5.801545319394553\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py # common package to interact with a dataset that is stored on an H5 file.\n",
    "import scipy\n",
    "# from PIL import Image\n",
    "from scipy import ndimage\n",
    "import os\n",
    "cwd= os.getcwd() # current working directory\n",
    "path = os.path.join(cwd,'data') \n",
    "# print (path)\n",
    "def load_dataset():\n",
    "\n",
    "    file_name=  os.path.join(path , 'train_catvnoncat.h5')\n",
    "    train_dataset = h5py.File(file_name, \"r\")\n",
    "    X_train = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    Y_train = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "    \n",
    "    file_name=  os.path.join(path , 'test_catvnoncat.h5')\n",
    "    test_dataset = h5py.File(file_name, \"r\")\n",
    "    X_test = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    Y_test = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = ['non-cat','cat']\n",
    "    \n",
    "    Y_train = Y_train.reshape(-1,1)\n",
    "    Y_test = Y_test.reshape(-1,1)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test, classes\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    g -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR_CODE. Implement sigmoid function\n",
    "    # START_CODE \n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    # END_CODE \n",
    "    \n",
    "    return g\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (1,dim) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (1,dim)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR_CODE.  Initialize b to zero and w as a vector of zeros. \n",
    "    # START_CODE   \n",
    "    w = np.zeros([1,dim])\n",
    "    b = 0.0\n",
    "    # END_CODE \n",
    "\n",
    "    assert(w.shape == (1,dim))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n",
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y, C=1):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (1,num_px * num_px * 3)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (number of examples,1)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    " \n",
    "    # YOUR_CODE.  implement forward propagation \n",
    "    # START_CODE   \n",
    "    Z= b + X @ w.T\n",
    "    A= sigmoid(Z)\n",
    "    cost = np.sum((Y * np.log(A)) + ((1 - Y) * np.log(1 - A))) / -m\n",
    "    \n",
    "    # END_CODE \n",
    "    \n",
    "    # YOUR_CODE.  Implement Backward propahation \n",
    "    # START_CODE   \n",
    "    dJ_dw = (1/m) * (A - Y).T @ X\n",
    "    dJ_db = np.sum(A - Y) / m\n",
    "\n",
    "   # END_CODE \n",
    "\n",
    "    assert(dJ_dw.shape == w.shape)\n",
    "    assert(dJ_db.dtype == float)\n",
    "    assert(cost.dtype == float)\n",
    "    \n",
    "    grads = {\"dJ_dw\": dJ_dw,\n",
    "             \"dJ_db\": dJ_db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, C= 1, verbose = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (1,num_px * num_px * 3)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (number of examples,1)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \"\"\"\n",
    "   \n",
    "    costs = [] # keep history for plotting if necessary \n",
    "    \n",
    "    for i in range(num_iterations):        \n",
    "\n",
    "    \n",
    "        # YOUR_CODE.  Call to compute cost and gradient \n",
    "        # START_CODE   \n",
    "        grads, cost = propagate(w,b,X,Y)\n",
    "        # END_CODE \n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dJ_dw = grads[\"dJ_dw\"]\n",
    "        dJ_db = grads[\"dJ_db\"]\n",
    "        \n",
    "        # YOUR_CODE.  Update paramaters \n",
    "        # START_CODE   \n",
    "        w = w-learning_rate*dJ_dw\n",
    "        b = b-learning_rate*dJ_db\n",
    "        # END_CODE \n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if verbose and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dJ_dw\": dJ_dw,\n",
    "             \"dJ_db\": dJ_db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "X_train,Y_train, X_test, Y_test, classes = load_dataset()\n",
    "\n",
    "# YOUR_CODE.   get m_train, num_px and m_test\n",
    "# START_CODE \n",
    "m_train = X_train.shape[0]\n",
    "num_px = X_test.shape[1]\n",
    "m_test = X_test.shape[0]\n",
    "# END_CODE \n",
    "\n",
    "# YOUR_CODE. Reshape the training and test set to shape (number_of_samples,  num_px*num_px*3)\n",
    "# START_CODE \n",
    "X_train_flatten = X_train.reshape(X_train.shape[0],-1)\n",
    "X_test_flatten =  X_test.reshape(X_test.shape[0],-1)\n",
    "# END_CODE\n",
    "X_train_scaled = X_train_flatten/255.\n",
    "X_test_scaled = X_test_flatten/255.\n",
    "\n",
    "w, b, X, Y = np.array([[1., 2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]).T, np.array([[1,0,1]]).T\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dJ_dw = \" + str(grads[\"dJ_dw\"]))\n",
    "print (\"dJ_db = \" + str(grads[\"dJ_db\"]))\n",
    "print (\"cost = \" + str(cost))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34bbadec9adf0ac853d67e6112bfc54e191411239a5bffbfa3fa4d3446ce0e03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
