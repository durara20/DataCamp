{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.584911\n",
      "Cost after iteration 200: 0.468157\n",
      "Cost after iteration 300: 0.377985\n",
      "Cost after iteration 400: 0.333494\n",
      "Cost after iteration 500: 0.305956\n",
      "Cost after iteration 600: 0.283224\n",
      "Cost after iteration 700: 0.264051\n",
      "Cost after iteration 800: 0.247613\n",
      "Cost after iteration 900: 0.233335\n",
      "Cost after iteration 1000: 0.220802\n",
      "Cost after iteration 1100: 0.209705\n",
      "Cost after iteration 1200: 0.199806\n",
      "Cost after iteration 1300: 0.190921\n",
      "Cost after iteration 1400: 0.182902\n",
      "Cost after iteration 1500: 0.175630\n",
      "Cost after iteration 1600: 0.169008\n",
      "Cost after iteration 1700: 0.162955\n",
      "Cost after iteration 1800: 0.157403\n",
      "Cost after iteration 1900: 0.152296\n",
      "Cost after iteration 2000: 0.147584\n",
      "Cost after iteration 2100: 0.143226\n",
      "Cost after iteration 2200: 0.139186\n",
      "Cost after iteration 2300: 0.135433\n",
      "Cost after iteration 2400: 0.131939\n",
      "Cost after iteration 2500: 0.128679\n",
      "Cost after iteration 2600: 0.125634\n",
      "Cost after iteration 2700: 0.122783\n",
      "Cost after iteration 2800: 0.120111\n",
      "Cost after iteration 2900: 0.117603\n",
      "train accuracy= 99.522%\n",
      "test accuracy= 68.000%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py # common package to interact with a dataset that is stored on an H5 file.\n",
    "import scipy\n",
    "# from PIL import Image\n",
    "from scipy import ndimage\n",
    "import os\n",
    "cwd= os.getcwd() # current working directory\n",
    "path = os.path.join(cwd,'data') \n",
    "# print (path)\n",
    "def load_dataset():\n",
    "\n",
    "    file_name=  os.path.join(path , 'train_catvnoncat.h5')\n",
    "    train_dataset = h5py.File(file_name, \"r\")\n",
    "    X_train = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    Y_train = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "    \n",
    "    file_name=  os.path.join(path , 'test_catvnoncat.h5')\n",
    "    test_dataset = h5py.File(file_name, \"r\")\n",
    "    X_test = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    Y_test = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = ['non-cat','cat']\n",
    "    \n",
    "    Y_train = Y_train.reshape(-1,1)\n",
    "    Y_test = Y_test.reshape(-1,1)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test, classes\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    g -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR_CODE. Implement sigmoid function\n",
    "    # START_CODE \n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    # END_CODE \n",
    "    \n",
    "    return g\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (1,dim) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (1,dim)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR_CODE.  Initialize b to zero and w as a vector of zeros. \n",
    "    # START_CODE   \n",
    "    w = np.zeros([1,dim])\n",
    "    b = 0.0\n",
    "    # END_CODE \n",
    "\n",
    "    assert(w.shape == (1,dim))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n",
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y, C=1):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (1,num_px * num_px * 3)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (number of examples,1)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    " \n",
    "    # YOUR_CODE.  implement forward propagation \n",
    "    # START_CODE   \n",
    "    Z = b + np.dot(X, w.T)\n",
    "    A = sigmoid(Z)\n",
    "    cost = (-1.0 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) + C/(2*m) * np.sum(w**2)\n",
    "    \n",
    "    dJ_dw = (1/m) * np.dot((A-Y).T, X) + C*w/m\n",
    "    dJ_db = (1/m) * np.sum(A - Y)\n",
    "\n",
    "   # END_CODE \n",
    "\n",
    "    assert(dJ_dw.shape == w.shape)\n",
    "    assert(dJ_db.dtype == float)\n",
    "    assert(cost.dtype == float)\n",
    "    \n",
    "    grads = {\"dJ_dw\": dJ_dw,\n",
    "             \"dJ_db\": dJ_db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, C= 1, verbose = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (1,num_px * num_px * 3)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (number of examples,1)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \"\"\"\n",
    "   \n",
    "    costs = [] # keep history for plotting if necessary \n",
    "    \n",
    "    for i in range(num_iterations):        \n",
    "\n",
    "    \n",
    "        # YOUR_CODE.  Call to compute cost and gradient \n",
    "        # START_CODE   \n",
    "        grads, cost = propagate(w,b,X,Y)\n",
    "        # END_CODE \n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dJ_dw = grads[\"dJ_dw\"]\n",
    "        dJ_db = grads[\"dJ_db\"]\n",
    "        \n",
    "        # YOUR_CODE.  Update paramaters \n",
    "        # START_CODE   \n",
    "        w = w-learning_rate*dJ_dw\n",
    "        b = b-learning_rate*dJ_db\n",
    "        # END_CODE \n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if verbose and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dJ_dw\": dJ_dw,\n",
    "             \"dJ_db\": dJ_db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "def predict(w, b, X):\n",
    "    def pred(a):\n",
    "        if a>0.5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights, a numpy array of size (1,num_px * num_px * 3)\n",
    "    b - bias, a scalar\n",
    "    X - data of size (number of examples, num_px * num_px * 3)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction - a numpy array of shape (number of examples, 1) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m,n = X.shape\n",
    "    assert (w.shape==(1,n))\n",
    " \n",
    "    # YOUR_CODE.  Compute \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    # START_CODE   \n",
    "    A= sigmoid(b+X@w.T)\n",
    "    # END_CODE \n",
    "\n",
    "    # YOUR_CODE.  Convert probabilities to actual predictions 0 or 1 \n",
    "    # START_CODE   \n",
    "    vector = np.vectorize(pred)\n",
    "    Y_prediction=vector(A)\n",
    "    # END_CODE \n",
    "    \n",
    "    assert(Y_prediction.shape == (m, 1))\n",
    "    \n",
    "    return Y_prediction\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, verbose = False, C = 1):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the functions implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (number of examples,1)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    C- regularization parameter \n",
    "    \n",
    "    Returns:\n",
    "    res -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR_CODE.\n",
    "    # START_CODE   \n",
    "\n",
    "    #  initialize parameters\n",
    "    dim = X_train.shape[1]\n",
    "    w, b = initialize_with_zeros(dim)\n",
    "    \n",
    "    # run gradient descent \n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, C, verbose)\n",
    "    \n",
    "    # retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters['w']\n",
    "    b = parameters['b']\n",
    "    \n",
    "    # predict test/train set examples\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "    # END_CODE \n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy= {:.3%}\".format(np.mean(Y_prediction_train == Y_train)))\n",
    "    print(\"test accuracy= {:.3%}\".format(np.mean(Y_prediction_test == Y_test)))\n",
    "    \n",
    "    res = {'costs': costs,\n",
    "           'Y_prediction_test': Y_prediction_test, \n",
    "           'Y_prediction_train' : Y_prediction_train, \n",
    "           'w' : w, \n",
    "           'b' : b,\n",
    "           'learning_rate' : learning_rate,\n",
    "           'num_iterations': num_iterations,\n",
    "           'C':C\n",
    "          }\n",
    "    \n",
    "    return res\n",
    "X_train,Y_train, X_test, Y_test, classes = load_dataset()\n",
    "\n",
    "# YOUR_CODE.   get m_train, num_px and m_test\n",
    "# START_CODE \n",
    "m_train = X_train.shape[0]\n",
    "num_px = X_test.shape[1]\n",
    "m_test = X_test.shape[0]\n",
    "# END_CODE \n",
    "\n",
    "# YOUR_CODE. Reshape the training and test set to shape (number_of_samples,  num_px*num_px*3)\n",
    "# START_CODE \n",
    "X_train_flatten = X_train.reshape(X_train.shape[0],-1)\n",
    "X_test_flatten =  X_test.reshape(X_test.shape[0],-1)\n",
    "# END_CODE\n",
    "X_train_scaled = X_train_flatten/255.\n",
    "X_test_scaled = X_test_flatten/255.\n",
    "\n",
    "res = model(X_train= X_train_scaled,\n",
    "            Y_train=Y_train, \n",
    "            X_test=X_test_scaled, \n",
    "            Y_test= Y_test, \n",
    "            num_iterations = 3000, \n",
    "            learning_rate = 0.005, \n",
    "            verbose = True,\n",
    "            C= 0.3 # 0.6 is still overfitting,   0.3  is low value to keep the test accuracy ashigh as possible\n",
    "           )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34bbadec9adf0ac853d67e6112bfc54e191411239a5bffbfa3fa4d3446ce0e03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
